"""
Chat API Routes
Handles all chat-related endpoints
"""

from fastapi import APIRouter, HTTPException, status, Depends
from sqlalchemy.orm import Session
from typing import Dict, List, Optional
import logging

from ..models.chat import ChatRequest, ChatResponse, ErrorResponse
from ..services.chat_service import get_chat_service
from ..services.basic_agent import run_basic_agent
from ..services.mcp_style_agent import run_mcp_agent
from ..database import get_db
from ..models.database import User, ChatHistory, ChatThread
from ..utils.auth import get_current_active_user
from ..api.threads import generate_thread_title

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["chat"])


@router.post(
    "/chat",
    response_model=ChatResponse,
    responses={
        500: {"model": ErrorResponse},
        400: {"model": ErrorResponse}
    },
    summary="Send a chat message",
    description="Send a message to the AI assistant and receive a response"
)
async def chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
) -> ChatResponse:
    """
    Chat endpoint - sends user message to Gemini and returns AI response
    Requires authentication and saves chat history to database
    
    **Request Body:**
    - message: User's message (required)
    - conversation_history: List of previous messages (optional)
    - thread_id: Thread ID to associate message with (optional)
    
    **Returns:**
    - AI assistant's response with timestamp and model info
    """
    try:
        logger.info(f"User {current_user.email} sent message with length: {len(request.message)}")
        
        # Handle thread creation or retrieval
        thread_id = request.thread_id
        is_new_thread = False
        
        if thread_id is None:
            # Create a new thread
            new_thread = ChatThread(user_id=current_user.id, title="New Conversation")
            db.add(new_thread)
            db.commit()
            db.refresh(new_thread)
            thread_id = new_thread.id
            is_new_thread = True
            logger.info(f"Created new thread {thread_id} for user {current_user.email}")
        else:
            # Verify thread belongs to user
            thread = db.query(ChatThread).filter(
                ChatThread.id == thread_id,
                ChatThread.user_id == current_user.id
            ).first()
            if not thread:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Thread not found"
                )
        
        # Get last 5 conversation pairs from this thread for conversation memory
        # Each ChatHistory record contains one user message and one AI response
        recent_messages = db.query(ChatHistory).filter(
            ChatHistory.thread_id == thread_id
        ).order_by(ChatHistory.created_at.desc()).limit(5).all()
        
        # Build conversation history (reverse to chronological order)
        # This will include up to 5 previous user messages and 5 previous AI responses
        history = []
        for msg in reversed(recent_messages):
            history.append({"role": "user", "content": msg.message})
            history.append({"role": "assistant", "content": msg.response})
        
        logger.info(f"Retrieved {len(recent_messages)} previous conversation pairs for thread {thread_id}")
        
        # Route to appropriate model based on request
        selected_model = request.model or "gemini"
        logger.info(f"ðŸŽ¯ Using model: {selected_model} for user {current_user.email}")
        
        if selected_model == "n8n":
            # Route to N8N multi-agent workflow
            import httpx
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        "http://localhost:5678/webhook/multi-agent",
                        json={"message": request.message}
                    )
                    response.raise_for_status()
                    n8n_result = response.json()
                    
                    result = {
                        "message": n8n_result.get("message", ""),
                        "model": "N8N Multi-Agent"
                    }
            except Exception as e:
                logger.error(f"N8N workflow error: {str(e)}")
                result = {
                    "message": f"N8N Multi-Agent workflow encountered an error: {str(e)}\\n\\nPlease ensure n8n is running and try again.",
                    "model": "N8N Multi-Agent (Error)"
                }
        elif selected_model == "mcp-style":
            # Use MCP Style Agent (Planner-Selector-Executor-Synthesizer)
            logger.info(f"ðŸ¤– Using MCP STYLE AGENT for user {current_user.email}")
            try:
                mcp_response = run_mcp_agent(request.message)
                result = {
                    "message": mcp_response,
                    "model": "MCP Style Agent"
                }
            except Exception as e:
                logger.error(f"MCP Style Agent error: {str(e)}")
                result = {
                    "message": f"MCP Style Agent encountered an error: {str(e)}\\n\\nPlease try again or select a different model.",
                    "model": "MCP Style Agent (Error)"
                }
        elif selected_model == "agent" or request.use_agent:
            # Use Basic Agent (ReAct pattern with tools)
            logger.info(f"ðŸ¤– Using BASIC AGENT for user {current_user.email}")
            agent_response = run_basic_agent(request.message)
            result = {
                "message": agent_response,
                "model": "Gemini Agent"
            }
        else:
            # Use regular Gemini model
            logger.info(f"ðŸ’¬ Using GEMINI for user {current_user.email}")
            chat_service = get_chat_service()
            
            # Check if thread has documents and should use RAG
            from app.services.rag_service import get_rag_service
            rag_service = get_rag_service()
            use_rag = rag_service.should_use_rag(current_user.id, thread_id)
            
            result = await chat_service.get_chat_response(
                user_message=request.message,
                conversation_history=history,
                user_id=current_user.id,
                thread_id=thread_id,
                use_rag=use_rag
            )
        
        # Save chat history to database with thread_id
        try:
            chat_record = ChatHistory(
                user_id=current_user.id,
                thread_id=thread_id,
                message=request.message,
                response=result["message"],
                model=result.get("model", "gemini-2.5-flash"),
                session_id=request.session_id
            )
            db.add(chat_record)
            db.commit()
            logger.info(f"Chat history saved for user {current_user.email} in thread {thread_id}")
        except Exception as db_error:
            logger.error(f"Failed to save chat history: {str(db_error)}")
            db.rollback()
            # Continue even if saving fails
        
        # Auto-generate/update thread title based on latest conversation
        try:
            # Generate title from the current conversation context
            title = await generate_thread_title(request.message)
            db.query(ChatThread).filter(ChatThread.id == thread_id).update({"title": title})
            db.commit()
            logger.info(f"{'Generated' if is_new_thread else 'Updated'} title '{title}' for thread {thread_id}")
        except Exception as title_error:
            logger.error(f"Failed to generate thread title: {str(title_error)}")
        
        # Return response with optional image data
        response_data = {
            "message": result["message"],
            "model": result.get("model", "gemini-2.5-flash"),
            "thread_id": thread_id
        }
        
        # Include image data if present
        if result.get("is_image"):
            response_data["image_url"] = result.get("image_url")
            response_data["is_image"] = True
        
        return ChatResponse(**response_data)
        
    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process chat request: {str(e)}"
        )


@router.get(
    "/chat/model-info",
    summary="Get model information",
    description="Get information about the current AI model configuration"
)
async def get_model_info() -> Dict[str, str]:
    """Get information about the current Gemini model"""
    try:
        chat_service = get_chat_service()
        return chat_service.get_model_info()
    except Exception as e:
        logger.error(f"Error getting model info: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


@router.post(
    "/agent/chat",
    summary="Simple agent endpoint for n8n",
    description="Lightweight endpoint that returns only the agent response (for n8n orchestration)"
)
async def agent_chat(
    request: ChatRequest,
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
) -> Dict:
    """
    Simplified chat endpoint for n8n integration.
    Returns only the agent response without UI logic.
    
    **Request Body:**
    - message: User's message (required)
    - thread_id: Thread ID (optional)
    - model: Model to use (optional, defaults to 'gemini')
    
    **Returns:**
    - response: Agent's text response
    - model: Model used
    - thread_id: Thread ID
    """
    try:
        logger.info(f"[N8N] User {current_user.email} sent message via n8n")
        
        # Handle thread
        thread_id = request.thread_id
        if thread_id is None:
            new_thread = ChatThread(user_id=current_user.id, title="N8N Conversation")
            db.add(new_thread)
            db.commit()
            db.refresh(new_thread)
            thread_id = new_thread.id
        
        # Get conversation history
        recent_messages = db.query(ChatHistory).filter(
            ChatHistory.thread_id == thread_id
        ).order_by(ChatHistory.created_at.desc()).limit(5).all()
        
        history = []
        for msg in reversed(recent_messages):
            history.append({"role": "user", "content": msg.message})
            history.append({"role": "assistant", "content": msg.response})
        
        # Route to appropriate agent
        selected_model = request.model or "gemini"
        
        if selected_model == "mcp-style":
            mcp_response = run_mcp_agent(request.message)
            response_text = mcp_response
            model_used = "MCP Style Agent"
        elif selected_model == "agent" or request.use_agent:
            response_text = run_basic_agent(request.message)
            model_used = "Gemini Agent"
        else:
            chat_service = get_chat_service()
            from app.services.rag_service import get_rag_service
            rag_service = get_rag_service()
            use_rag = rag_service.should_use_rag(current_user.id, thread_id)
            
            result = await chat_service.get_chat_response(
                user_message=request.message,
                conversation_history=history,
                user_id=current_user.id,
                thread_id=thread_id,
                use_rag=use_rag
            )
            response_text = result["message"]
            model_used = result.get("model", "gemini-2.5-flash-lite")
        
        # Save to database
        chat_record = ChatHistory(
            user_id=current_user.id,
            thread_id=thread_id,
            message=request.message,
            response=response_text,
            model=model_used,
            session_id=request.session_id
        )
        db.add(chat_record)
        db.commit()
        
        logger.info(f"[N8N] Chat history saved for user {current_user.email} in thread {thread_id}")
        
        # Return simple response for n8n
        return {
            "response": response_text,
            "model": model_used,
            "thread_id": thread_id,
            "user_id": current_user.id
        }
        
    except Exception as e:
        logger.error(f"[N8N] Error in agent_chat: {str(e)}")
        # Return error as JSON (n8n can handle this)
        return {
            "response": "I encountered an error processing your request. Please try again.",
            "error": str(e),
            "model": "error",
            "thread_id": request.thread_id
        }


@router.get("/history")
async def get_chat_history(
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db),
    limit: int = 50,
    session_id: str = None
):
    """Get chat history for the current user"""
    try:
        query = db.query(ChatHistory).filter(ChatHistory.user_id == current_user.id)
        
        if session_id:
            query = query.filter(ChatHistory.session_id == session_id)
        
        chat_history = query.order_by(ChatHistory.created_at.desc()).limit(limit).all()
        
        return {
            "history": [
                {
                    "id": chat.id,
                    "message": chat.message,
                    "response": chat.response,
                    "model": chat.model,
                    "session_id": chat.session_id,
                    "created_at": chat.created_at
                }
                for chat in chat_history
            ]
        }
    except Exception as e:
        logger.error(f"Error getting chat history: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve chat history"
        )

